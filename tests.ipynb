{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-07T23:48:44.234652Z",
     "start_time": "2024-12-07T23:48:44.230094Z"
    }
   },
   "source": [
    "import json\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import logging\n",
    "from urllib.parse import urlparse\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Настройка логирования\n",
    "logging.basicConfig(\n",
    "    filename='scraper.log',\n",
    "    filemode='a',\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    level=logging.INFO\n",
    ")\n",
    "\n",
    "# Константы\n",
    "URLS_JSON_PATH = 'urls.json'  # Путь к вашему JSON файлу со списком URL\n",
    "HTML_SAVE_DIR = 'downloaded_html'\n",
    "PARSED_SAVE_DIR = 'parsed_data'\n",
    "LOG_FILE = 'scraper.log'\n",
    "\n",
    "# Создание директорий для сохранения, если они не существуют\n",
    "os.makedirs(HTML_SAVE_DIR, exist_ok=True)\n",
    "os.makedirs(PARSED_SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# Функция для генерации безопасного имени файла из URL\n",
    "def generate_filename(url):\n",
    "    parsed_url = urlparse(url)\n",
    "    filename = parsed_url.path.replace('/', '_').strip('_')\n",
    "    if not filename:\n",
    "        filename = 'root'\n",
    "    # Ограничение длины имени файла\n",
    "    return f\"{filename[:150]}.html\"\n",
    "\n",
    "# Загрузка списка URL из JSON файла\n",
    "try:\n",
    "    with open(URLS_JSON_PATH, 'r', encoding='utf-8') as f:\n",
    "        urls = json.load(f)\n",
    "    logging.info(f\"Загружено {len(urls)} URL из {URLS_JSON_PATH}\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"Ошибка при загрузке URL из JSON: {e}\")\n",
    "    raise\n",
    "\n",
    "# Выбор случайных 1/10 URL\n",
    "sample_size = max(1, len(urls) // 10)\n",
    "sample_urls = random.sample(urls, sample_size)\n",
    "logging.info(f\"Выбрано {len(sample_urls)} случайных URL для обработки\")\n",
    "\n",
    "# Настройка заголовков для имитации реального браузера\n",
    "HEADERS = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) '\n",
    "                  'AppleWebKit/537.36 (KHTML, like Gecko) '\n",
    "                  'Chrome/112.0.0.0 Safari/537.36'\n",
    "}\n",
    "\n",
    "# Функция для загрузки HTML с обработкой ошибок и задержками\n",
    "def fetch_html(url):\n",
    "    try:\n",
    "        response = requests.get(url, headers=HEADERS, timeout=10)\n",
    "        if response.status_code == 200:\n",
    "            return response.text\n",
    "        elif response.status_code == 403:\n",
    "            logging.warning(f\"Получен 403 для URL: {url}. Пауза на 5.5 минут.\")\n",
    "            time.sleep(5.5 * 60)  # Пауза 5.5 минут\n",
    "            # После паузы можно попробовать снова или вернуть None\n",
    "            return None\n",
    "        else:\n",
    "            logging.error(f\"Неожиданный статус-код {response.status_code} для URL: {url}\")\n",
    "            return None\n",
    "    except requests.RequestException as e:\n",
    "        logging.error(f\"Запрос к URL {url} завершился ошибкой: {e}\")\n",
    "        return None\n",
    "\n",
    "# Функция для парсинга HTML и извлечения таблиц\n",
    "def parse_html(html_content):\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    tables = soup.find_all('table')\n",
    "\n",
    "    extracted_tables = []\n",
    "\n",
    "    for tab in tables:\n",
    "        # Пытаемся найти caption\n",
    "        caption_tag = tab.find('caption')\n",
    "        caption = caption_tag.get_text(strip=True) if caption_tag else None\n",
    "\n",
    "        # Пытаемся определить заголовки столбцов:\n",
    "        headers = []\n",
    "        thead = tab.find('thead')\n",
    "        if thead:\n",
    "            header_row = thead.find('tr')\n",
    "            if header_row:\n",
    "                headers = [th.get_text(strip=True) for th in header_row.find_all(['th','td'])]\n",
    "        else:\n",
    "            # Если нет thead, берем первую tr\n",
    "            first_row = tab.find('tr')\n",
    "            if first_row:\n",
    "                headers = [th.get_text(strip=True) for th in first_row.find_all(['th','td'])]\n",
    "\n",
    "        # Извлекаем все остальные строки\n",
    "        rows = []\n",
    "        all_tr = tab.find_all('tr')\n",
    "        # Пропускаем первую строку если она заголовочная\n",
    "        data_rows = all_tr[1:] if len(all_tr) > 1 else []\n",
    "\n",
    "        for r in data_rows:\n",
    "            cells = [td.get_text(strip=True) for td in r.find_all(['td','th'])]\n",
    "            rows.append(cells)\n",
    "\n",
    "        extracted_tables.append({\n",
    "            \"caption\": caption,\n",
    "            \"headers\": headers,\n",
    "            \"rows\": rows,\n",
    "        })\n",
    "\n",
    "    return extracted_tables\n",
    "\n",
    "# Основной цикл обработки URL\n",
    "for url in tqdm(sample_urls, desc=\"Обработка URL\"):\n",
    "    filename = generate_filename(url)\n",
    "    html_path = os.path.join(HTML_SAVE_DIR, filename)\n",
    "    parsed_path = os.path.join(PARSED_SAVE_DIR, f\"{filename}.json\")\n",
    "\n",
    "    # Проверка, был ли уже загружен этот URL\n",
    "    if os.path.exists(parsed_path):\n",
    "        logging.info(f\"Пропуск уже обработанного URL: {url}\")\n",
    "        continue\n",
    "\n",
    "    html_content = fetch_html(url)\n",
    "    if html_content:\n",
    "        try:\n",
    "            # Сохранение HTML на диск\n",
    "            with open(html_path, 'w', encoding='utf-8') as f:\n",
    "                f.write(html_content)\n",
    "            logging.info(f\"Сохранен HTML для URL: {url} в файл {html_path}\")\n",
    "\n",
    "            # Парсинг HTML\n",
    "            parsed_data = parse_html(html_content)\n",
    "\n",
    "            # Сохранение распарсенных данных\n",
    "            with open(parsed_path, 'w', encoding='utf-8') as f:\n",
    "                json.dump({\n",
    "                    \"url\": url,\n",
    "                    \"parsed_tables\": parsed_data\n",
    "                }, f, ensure_ascii=False, indent=4)\n",
    "            logging.info(f\"Сохранены распарсенные данные для URL: {url} в файл {parsed_path}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Ошибка при обработке URL {url}: {e}\")\n",
    "    else:\n",
    "        logging.warning(f\"Не удалось получить контент для URL: {url}\")\n",
    "\n",
    "    # Задержка между запросами (например, от 1 до 3 секунд)\n",
    "    delay = random.uniform(4, 12)\n",
    "    time.sleep(delay)\n",
    "\n",
    "logging.info(\"Завершение обработки всех URL\")"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-07T23:49:56.168911Z",
     "start_time": "2024-12-07T23:49:55.105527Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "response = requests.get(url)\n",
    "html_content = response.text\n",
    "\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "tables = soup.find_all('table')"
   ],
   "id": "bd80b365f61a1335",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-07T23:50:47.429123Z",
     "start_time": "2024-12-07T23:50:47.424308Z"
    }
   },
   "cell_type": "code",
   "source": [
    "extracted_tables = []\n",
    "\n",
    "for tab in tables:\n",
    "    # Пытаемся найти caption\n",
    "    caption_tag = tab.find('caption')\n",
    "    caption = caption_tag.get_text(strip=True) if caption_tag else None\n",
    "    \n",
    "    # Пытаемся определить заголовки столбцов:\n",
    "    # Ищем thead или берем первую строку tr\n",
    "    headers = []\n",
    "    thead = tab.find('thead')\n",
    "    if thead:\n",
    "        header_row = thead.find('tr')\n",
    "        if header_row:\n",
    "            headers = [th.get_text(strip=True) for th in header_row.find_all(['th','td'])]\n",
    "    else:\n",
    "        # Если нет thead, берем первую tr в tbody или напрямую в table\n",
    "        first_row = tab.find('tr')\n",
    "        if first_row:\n",
    "            headers = [th.get_text(strip=True) for th in first_row.find_all(['th','td'])]\n",
    "    \n",
    "    # Извлекаем все остальные строки\n",
    "    rows = []\n",
    "    all_tr = tab.find_all('tr')\n",
    "    # Пропускаем первую строку если она заголовочная\n",
    "    data_rows = all_tr[1:] if len(all_tr) > 1 else []\n",
    "    \n",
    "    for r in data_rows:\n",
    "        cells = [td.get_text(strip=True) for td in r.find_all(['td','th'])]\n",
    "        rows.append(cells)\n",
    "    \n",
    "    extracted_tables.append({\n",
    "        \"caption\": caption,\n",
    "        \"headers\": headers,\n",
    "        \"rows\": rows,\n",
    "    })"
   ],
   "id": "74fbdc27681a4623",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-07T23:50:54.105697Z",
     "start_time": "2024-12-07T23:50:54.101342Z"
    }
   },
   "cell_type": "code",
   "source": "extracted_tables",
   "id": "9095cb6fd87db717",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'caption': None,\n",
       "  'headers': ['Реквизиты корпоративного действия'],\n",
       "  'rows': [['Референс корпоративного действия', '806764'],\n",
       "   ['Код типа корпоративного действия', 'MEET'],\n",
       "   ['Тип корпоративного действия', 'Годовое общее собрание акционеров'],\n",
       "   ['Дата КД (факт.)', '06 июня 2023 г.'],\n",
       "   ['Дата фиксации', '12 мая 2023 г.'],\n",
       "   ['Форма проведения собрания', 'Заочная']]},\n",
       " {'caption': None,\n",
       "  'headers': ['Информация о ценных бумагах'],\n",
       "  'rows': [['Референс КД по ценной бумаге',\n",
       "    'Эмитент',\n",
       "    'Регистрационный номер',\n",
       "    'Дата регистрации',\n",
       "    'Категория',\n",
       "    'Депозитарный код выпуска',\n",
       "    'ISIN',\n",
       "    'Реестродержатель'],\n",
       "   ['806764X23120',\n",
       "    'Акционерное общество \"Ульяновская сетевая компания\"',\n",
       "    '1-01-03826-E',\n",
       "    '02 октября 2006 г.',\n",
       "    'акции обыкновенные',\n",
       "    'RU000A0JUY39',\n",
       "    'RU000A0JUY39',\n",
       "    'АО \"НРК - Р.О.С.Т.\"']]}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": 12,
   "source": [
    "extracted_tables\n",
    "\n",
    "# Сохраняем результат в JSON для удобства дальнейшей ручной разметки\n",
    "import json\n",
    "with open(\"parsed_tables.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(extracted_tables, f, ensure_ascii=False, indent=2)\n"
   ],
   "id": "e9179304aa62c1b9"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
